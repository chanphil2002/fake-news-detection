{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chanphil2002/fake-news-detection/blob/main/FakeNews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jsJlTuhRmDQL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "fake_file_path = '/content/drive/MyDrive/Colab Notebooks/data/Fake.csv'\n",
        "real_file_path = '/content/drive/MyDrive/Colab Notebooks/data/True.csv'\n",
        "fake = pd.read_csv(fake_file_path)\n",
        "real = pd.read_csv(real_file_path)\n",
        "\n",
        "# Check the first few rows of the dataset\n",
        "print(fake.head())\n",
        "print(real.head())\n",
        "\n",
        "# Add labels: 0 for fake, 1 for real\n",
        "fake['label'] = 0\n",
        "real['label'] = 1\n",
        "\n",
        "# Combine datasets\n",
        "data = pd.concat([fake, real], ignore_index=True)\n",
        "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Merge title and text into a single field\n",
        "data['content'] = data['title'].fillna('') + \" \" + data['text'].fillna('')\n",
        "data = data[['content', 'label']]\n",
        "\n",
        "# Basic overview\n",
        "print(data.head())\n",
        "print(data['label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gWhHfP1naS7"
      },
      "outputs": [],
      "source": [
        "# Class balance\n",
        "sns.countplot(data=data, x='label')\n",
        "plt.xticks([0, 1], ['Fake', 'Real'])\n",
        "plt.title(\"Class Distribution\")\n",
        "plt.show()\n",
        "\n",
        "# Add length feature\n",
        "data['content_len'] = data['content'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Article length by class\n",
        "sns.histplot(data=data, x='content_len', hue='label', bins=50, kde=True)\n",
        "plt.title(\"Article Length Distribution by Class\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhnfhP-8nb5z"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "fake_text = \" \".join(data[data['label']==0]['content'].tolist())\n",
        "real_text = \" \".join(data[data['label']==1]['content'].tolist())\n",
        "\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(WordCloud(width=800, height=400, background_color='white').generate(fake_text))\n",
        "plt.title(\"Fake News WordCloud\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(WordCloud(width=800, height=400, background_color='white').generate(real_text))\n",
        "plt.title(\"Real News WordCloud\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sx_iyDJ3nj5q"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "!pip install keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apa8EL_Ynma4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Text cleaning function\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)           # Remove text in brackets\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)     # Remove URLs\n",
        "    text = re.sub(r'<.*?>+', '', text)             # Remove HTML tags\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)           # Remove punctuation/numbers\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Apply cleaning\n",
        "data['clean_content'] = data['content'].apply(clean_text)\n",
        "\n",
        "# Parameters\n",
        "MAX_WORDS = 15000\n",
        "MAX_LEN = 300\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
        "tokenizer.fit_on_texts(data['clean_content'])\n",
        "\n",
        "X = tokenizer.texts_to_sequences(data['clean_content'])\n",
        "X = pad_sequences(X, maxlen=MAX_LEN)\n",
        "\n",
        "y = data['label'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eUJZYx2nsZx"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvCuTCjTntjO"
      },
      "outputs": [],
      "source": [
        "import keras_tuner as kt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D, Bidirectional, SpatialDropout1D\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "EMBEDDING_DIM = 128\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jp-hj7qyoJoE"
      },
      "outputs": [],
      "source": [
        "def build_lstm_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=MAX_WORDS,\n",
        "                        output_dim=hp.Choice('embedding_dim', [64, 128, 256]),\n",
        "                        input_length=MAX_LEN))\n",
        "    model.add(SpatialDropout1D(hp.Float('spatial_dropout', 0.1, 0.5, step=0.1)))\n",
        "    model.add(Bidirectional(LSTM(units=hp.Int('lstm_units_1', 32, 128, step=32),\n",
        "                                 return_sequences=True,\n",
        "                                 dropout=hp.Float('dropout_1', 0.2, 0.5, step=0.1),\n",
        "                                 recurrent_dropout=hp.Float('recurrent_dropout_1', 0.2, 0.5, step=0.1))))\n",
        "    model.add(Bidirectional(LSTM(units=hp.Int('lstm_units_2', 32, 128, step=32),\n",
        "                                 dropout=hp.Float('dropout_2', 0.2, 0.5, step=0.1),\n",
        "                                 recurrent_dropout=hp.Float('recurrent_dropout_2', 0.2, 0.5, step=0.1))))\n",
        "    model.add(Dense(hp.Int('dense_units_1', 64, 256, step=64), activation='relu'))\n",
        "    model.add(Dropout(hp.Float('dense_dropout_1', 0.2, 0.5, step=0.1)))\n",
        "    model.add(Dense(hp.Int('dense_units_2', 64, 256, step=64), activation='relu'))\n",
        "    model.add(Dropout(hp.Float('dense_dropout_2', 0.2, 0.5, step=0.1)))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer=RMSprop(learning_rate=hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "lstm_tuner = kt.Hyperband(\n",
        "    build_lstm_model,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=5,\n",
        "    factor=3,\n",
        "    directory='hyperband_logs',\n",
        "    project_name='lstm_hyperband'\n",
        ")\n",
        "\n",
        "# Use a smaller subset of training data for faster tuning\n",
        "X_tune = X_train[:8000]\n",
        "y_tune = y_train[:8000]\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=2)\n",
        "\n",
        "lstm_tuner.search(X_tune, y_tune,\n",
        "             epochs=5,\n",
        "             validation_split=0.2,\n",
        "             callbacks=[early_stop])\n",
        "\n",
        "best_lstm_model = lstm_tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "history = best_lstm_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)]\n",
        ")\n",
        "\n",
        "best_lstm_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nd97ecMloKNb"
      },
      "outputs": [],
      "source": [
        "def build_cnn_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=MAX_WORDS,\n",
        "                        output_dim=hp.Choice('embedding_dim', [64, 128, 256]),\n",
        "                        input_length=MAX_LEN))\n",
        "\n",
        "    model.add(Conv1D(filters=hp.Int('filters_1', 64, 256, step=64), kernel_size=3, activation='relu'))\n",
        "    model.add(Conv1D(filters=hp.Int('filters_2', 64, 256, step=64), kernel_size=3, activation='relu'))\n",
        "\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(Dense(hp.Int('dense_units_1', 64, 256, step=64), activation='relu'))\n",
        "    model.add(Dropout(hp.Float('dropout_1', 0.2, 0.5, step=0.1)))\n",
        "    model.add(Dense(hp.Int('dense_units_2', 64, 256, step=64), activation='relu'))\n",
        "    model.add(Dropout(hp.Float('dropout_2', 0.2, 0.5, step=0.1)))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer=RMSprop(learning_rate=hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "cnn_tuner = kt.Hyperband(\n",
        "    build_cnn_model,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=10,\n",
        "    factor=3,\n",
        "    directory='hyperband_logs',\n",
        "    project_name='cnn_hyperband'\n",
        ")\n",
        "\n",
        "\n",
        "cnn_tuner.search(X_train, y_train,\n",
        "                 epochs=10,\n",
        "                 validation_split=0.2,\n",
        "                 callbacks=[EarlyStopping(monitor='val_loss', patience=2)])\n",
        "\n",
        "best_cnn_model = cnn_tuner.get_best_models(num_models=1)[0]\n",
        "best_cnn_model.summary()\n",
        "\n",
        "# Retrain best model\n",
        "history_cnn = best_cnn_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YJZOuDAofDw"
      },
      "outputs": [],
      "source": [
        "# Evaluate LSTM\n",
        "loss_lstm, accuracy_lstm = best_lstm_model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"LSTM Test Accuracy: {accuracy_lstm:.4f}\")\n",
        "\n",
        "# Evaluate CNN\n",
        "loss_cnn, accuracy_cnn = best_cnn_model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"CNN Test Accuracy: {accuracy_cnn:.4f}\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_history(history, title):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "    plt.title(f'{title} - Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "    plt.title(f'{title} - Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot LSTM\n",
        "plot_history(history_lstm, 'LSTM')\n",
        "\n",
        "# Plot CNN\n",
        "plot_history(history_cnn, 'CNN')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNiynWoKes01usUd4vr5N86",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}